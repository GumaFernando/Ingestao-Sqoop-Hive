# Ingestão de dados no hive utilizando o Scoop

Através dos conhecimentos adquiridos no Bootcamp de engenharia de dados da Semantix. Realizei uma prática de importação de uma tabela no banco de dados
do MYSQL diretamente para o Hive, realizando o armazenamento no HDFS no formato Parquet.

### Verificando quantidade total de registros na tabela do MYSQL - Total - 300024

![alt text](https://github.com/GumaFernando/Projeto_Ingestao_Sqoop/blob/main/mysql_count_total.PNG?raw=true)

### Criando Database > Guma no hive 
![alt text](https://github.com/GumaFernando/Projeto_Ingestao_Sqoop/blob/main/create_database_hive.PNG)

### Importando os dados do Mysql diretamente no HIVE
![alt text](https://github.com/GumaFernando/Projeto_Ingestao_Sqoop/blob/main/sqoop_import.PNG)

### Validando registros no Hive ! Total - 300024

![alt text](https://github.com/GumaFernando/Projeto_Ingestao_Sqoop/blob/main/consulta_hive.PNG)

### Verificando arquivos no formato Parquet no hdfs 

![alt text](https://github.com/GumaFernando/Projeto_Ingestao_Sqoop/blob/main/consulta_hdfs.PNG)
